{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36f157a6-574e-4e85-b26f-d5a4ce39912a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X:\n",
      "4.63671e+000     1932268\n",
      "5.16978e-001     1932268\n",
      "-3.20594e+000    1932268\n",
      "1.82241e+000     1932268\n",
      "Unnamed: 4       2021099\n",
      "                  ...   \n",
      "-2.16663e+000    1910765\n",
      "-5.51068e+000    1915372\n",
      "4.80685e+000     1915372\n",
      "-1.42651e+000    1915372\n",
      "5.08883e-001     1915372\n",
      "Length: 81, dtype: int64\n",
      "Missing values in y:\n",
      "0\n",
      "Shape of X: (2021099, 81)\n",
      "Shape of y: (2021099,)\n",
      "Data types in X:\n",
      "4.63671e+000     float64\n",
      "5.16978e-001     float64\n",
      "-3.20594e+000    float64\n",
      "1.82241e+000     float64\n",
      "Unnamed: 4       float64\n",
      "                  ...   \n",
      "-2.16663e+000    float64\n",
      "-5.51068e+000    float64\n",
      "4.80685e+000     float64\n",
      "-1.42651e+000    float64\n",
      "5.08883e-001     float64\n",
      "Length: 81, dtype: object\n",
      "Data types in y:\n",
      "int64\n",
      "Shape of X_train: (1616879, 81)\n",
      "Shape of y_train: (1616879,)\n",
      "Error during model fitting: Input X contains NaN.\n",
      "RandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values\n",
      "X_train sample:          4.63671e+000  5.16978e-001  -3.20594e+000  1.82241e+000  Unnamed: 4  \\\n",
      "206438      -0.022231       0.00702      -0.039011     -0.007286         NaN   \n",
      "1163227     -0.022231       0.00702      -0.039011     -0.007286         NaN   \n",
      "584558      -0.022231       0.00702      -0.039011     -0.007286         NaN   \n",
      "614252      -0.022231       0.00702      -0.039011     -0.007286         NaN   \n",
      "240092      -0.022231       0.00702      -0.039011     -0.007286         NaN   \n",
      "\n",
      "         -1.69381e-001  -1.28208e+000  3.30282e+000  -1.55699e+000  \\\n",
      "206438        0.037411      -0.014566      0.052197      -0.013185   \n",
      "1163227       0.037411      -0.014566      0.052197      -0.013185   \n",
      "584558        0.037411      -0.014566      0.052197      -0.013185   \n",
      "614252        0.037411      -0.014566      0.052197      -0.013185   \n",
      "240092        0.037411      -0.014566      0.052197      -0.013185   \n",
      "\n",
      "         -7.82861e+000  ...  1.53455e+000  -1.01746e+000  8.31242e+000  \\\n",
      "206438       -0.252089  ...     -0.006339      -0.001528      0.002195   \n",
      "1163227      -0.030497  ...     -0.006339      -0.001528      0.002195   \n",
      "584558       -0.030497  ...     -0.006339      -0.001528      0.002195   \n",
      "614252       -0.030497  ...     -0.006339      -0.001528      0.002195   \n",
      "240092        2.156830  ...     -0.006339      -0.001528      0.002195   \n",
      "\n",
      "         -4.66927e+000  -2.79574e-001  -2.16663e+000  -5.51068e+000  \\\n",
      "206438        0.000154       0.003055       0.020582       0.006528   \n",
      "1163227       0.000154       0.003055       0.020582       0.006528   \n",
      "584558        0.000154       0.003055       0.020582       0.006528   \n",
      "614252        0.000154       0.003055       0.020582       0.006528   \n",
      "240092        0.000154       0.003055       0.020582       0.006528   \n",
      "\n",
      "         4.80685e+000  -1.42651e+000  5.08883e-001  \n",
      "206438      -0.002016       0.010261     -0.009582  \n",
      "1163227     -0.002016       0.010261     -0.009582  \n",
      "584558      -0.002016       0.010261     -0.009582  \n",
      "614252      -0.002016       0.010261     -0.009582  \n",
      "240092      -0.002016       0.010261     -0.009582  \n",
      "\n",
      "[5 rows x 81 columns]\n",
      "y_train sample: 206438     0\n",
      "1163227    1\n",
      "584558     0\n",
      "614252     0\n",
      "240092     0\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m rf_model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError during model fitting:\u001b[39m\u001b[38;5;124m\"\u001b[39m, e)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:345\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 345\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    346\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    347\u001b[0m )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    349\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\Lib\\site-packages\\sklearn\\base.py:584\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    583\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 584\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    585\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1107\u001b[0m     X,\n\u001b[0;32m   1108\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1109\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1110\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1111\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1112\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1113\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1114\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1115\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1116\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1117\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1118\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1119\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1120\u001b[0m )\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:921\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    916\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    917\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    918\u001b[0m         )\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 921\u001b[0m         _assert_all_finite(\n\u001b[0;32m    922\u001b[0m             array,\n\u001b[0;32m    923\u001b[0m             input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    924\u001b[0m             estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    925\u001b[0m             allow_nan\u001b[38;5;241m=\u001b[39mforce_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    926\u001b[0m         )\n\u001b[0;32m    928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    929\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32mD:\\software\\Anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nRandomForestClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Define paths to healthy and broken-tooth data folders\n",
    "healthy_data_folder = r'C:\\Users\\found\\all anaconda\\Untitled Folder 1\\healthy_data_folder'\n",
    "broken_tooth_data_folder = r'C:\\Users\\found\\all anaconda\\Untitled Folder 1\\broken_tooth_data_folder'\n",
    "\n",
    "# Function to load data from .txt files\n",
    "def load_data(folder, label, delimiter='\\t'):  # Adjust delimiter if necessary\n",
    "    data = []\n",
    "    for file in os.listdir(folder):\n",
    "        file_path = os.path.join(folder, file)\n",
    "        \n",
    "        # Read the .txt file (adjust delimiter if needed)\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter)  # Use correct delimiter\n",
    "        \n",
    "        # Add a label column to indicate the class (0 = healthy, 1 = broken tooth)\n",
    "        df['label'] = label\n",
    "        data.append(df)\n",
    "    \n",
    "    return pd.concat(data, ignore_index=True)\n",
    "\n",
    "# Load healthy and broken-tooth data\n",
    "healthy_data = load_data(healthy_data_folder, label=0, delimiter='\\t')  # Adjust delimiter if needed\n",
    "broken_tooth_data = load_data(broken_tooth_data_folder, label=1, delimiter='\\t')  # Adjust delimiter\n",
    "\n",
    "# Combine the two datasets into one\n",
    "data = pd.concat([healthy_data, broken_tooth_data], ignore_index=True)\n",
    "\n",
    "# Define features and target\n",
    "X = data.drop(columns=['label'])  # Drop the 'label' column to use the remaining columns as features\n",
    "y = data['label']  # Use the 'label' column as the target\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in X:\")\n",
    "print(X.isnull().sum())\n",
    "print(\"Missing values in y:\")\n",
    "print(y.isnull().sum())\n",
    "\n",
    "# Check shapes\n",
    "print(f\"Shape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")\n",
    "\n",
    "# Handle missing values (if any)\n",
    "X = X.fillna(X.mean())  # Fill missing values with the mean\n",
    "y = y.fillna(y.mode()[0])  # Fill missing values in the target with the mode\n",
    "\n",
    "# Ensure categorical features are properly encoded (if any)\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Check the data types\n",
    "print(\"Data types in X:\")\n",
    "print(X.dtypes)\n",
    "print(\"Data types in y:\")\n",
    "print(y.dtypes)\n",
    "\n",
    "# Train-test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check shapes of train sets\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")\n",
    "\n",
    "# Fit the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "try:\n",
    "    rf_model.fit(X_train, y_train)\n",
    "except ValueError as e:\n",
    "    print(\"Error during model fitting:\", e)\n",
    "    print(\"X_train sample:\", X_train.head())\n",
    "    print(\"y_train sample:\", y_train.head())\n",
    "    raise  # Re-raise the error to show it in the output\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display predictions alongside actual values for comparison\n",
    "results = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "print(\"Predictions vs Actual:\")\n",
    "print(results.head(10))  # Display the first 10 rows of actual vs predicted\n",
    "\n",
    "# Example input data for prediction (adjust feature values accordingly)\n",
    "feature_names = X.columns.tolist()  # Get feature names from the DataFrame\n",
    "new_data = np.array([[0.5, 1.5, 2.0, 3.5, 4.0]])  # Adjust this array as needed\n",
    "\n",
    "# Create a DataFrame for the new data\n",
    "new_data_df = pd.DataFrame(new_data, columns=feature_names)\n",
    "\n",
    "# Make the prediction\n",
    "predicted_label = rf_model.predict(new_data_df)\n",
    "\n",
    "# Output the prediction result\n",
    "if predicted_label[0] == 0:\n",
    "    print(\"The prediction for the input data is: Healthy\")\n",
    "else:\n",
    "    print(\"The prediction for the input data is: Broken Tooth\")\n",
    "\n",
    "# Optional: Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance = pd.DataFrame({'feature': X.columns, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "print(\"Feature Importances:\")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a979ff-aac0-41d4-9654-d25898909fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc651b-425b-4801-8e39-e3baef14a29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2060802-369f-4ca3-b49f-66edbb749f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
